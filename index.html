<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Safety Issues for Generative AI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>COLING 2025 Tutorial: Safety Issues for Generative AI</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">
              <span style="font-size: 80%">COLING 2025 Tutorial</span><br />
              Safety Issues for Generative AI
            </h1>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <div style="display: flex; justify-content: space-between; padding: 5px">
                  <figure style="display: inline-block; width: 19%;">
                    <img class="center" style="height: auto;" src="static/imgs/haonan_pic.jpg">
                    <figcaption class="is-size-6-mobile"><a href="#" style="border-radius: 20%" target="_blank">Haonan
                        Li</a><sup>1,2</sup></figcaption>
                  </figure>
                  <figure style="display: inline-block; width: 19%;">
                    <img class="center" style="height: auto;" src="static/imgs/xudong_pic.jpg">
                    <figcaption class="is-size-6-mobile"><a href="#" style="border-radius: 50%" target="_blank">Xudong
                        Han</a><sup>1,2</sup></figcaption>
                  </figure>
                  <figure style="display: inline-block; width: 19%;">
                    <img class="center" style="height: auto;" src="static/imgs/emad_pic.jpg">
                    <figcaption class="is-size-6-mobile"><a href="#" style="border-radius: 50%" target="_blank">Emad
                        Alghamdi</a><sup>1,3</sup></figcaption>
                  </figure>
                  <figure style="display: inline-block; width: 19%;">
                    <img class="center" style="height: auto;" src="static/imgs/lizhi_pic.jpg">
                    <figcaption class="is-size-6-mobile"><a href="#" style="border-radius: 50%" target="_blank">Lizhi
                        Lin</a><sup>4</sup></figcaption>
                  </figure>
                  <figure style="display: inline-block; width: 19%;">
                    <img class="center" style="height: auto;" src="static/imgs/monojit_pic.jpg">
                    <figcaption class="is-size-6-mobile"><a href="#" style="border-radius: 50%" target="_blank">Monojit
                        Choudhury</a><sup>1</sup></figcaption>
                  </figure>
                </div>
                <div style="display: flex; justify-content: space-between; padding: 5px">
                  <figure style="display: inline-block; width: 33%;">
                    <img class="center" style="height: auto;" src="static/imgs/jingfeng_pic.jpg">
                    <figcaption class="is-size-6-mobile"><a href="#" style="border-radius: 50%" target="_blank">Jingfeng
                        Zhang</a><sup>5</sup></figcaption>
                  </figure>
                  <figure style="display: inline-block; width: 33%;">
                    <img class="center" style="height: auto;" src="static/imgs/paul_pic.jpg">
                    <figcaption class="is-size-6-mobile"><a href="#" style="border-radius: 50%" target="_blank">Paul
                        Röttger</a><sup>6</sup></figcaption>
                  </figure>
                  <figure style="display: inline-block; width: 33%;">
                    <img class="center" style="height: auto;" src="static/imgs/timothy_pic.jpg">
                    <figcaption class="is-size-6-mobile"><a href="#" style="border-radius: 50%" target="_blank">Timothy
                        Baldwin</a><sup>1,2,7</sup></figcaption>
                  </figure>
                </div>
              </span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>1</sup>MBZUAI</span>
              <span class="author-block"><sup>2</sup>LibrAI</span>
              <span class="author-block"><sup>3</sup>King Abdulaziz University</span>
              <span class="author-block"><sup>4</sup>Tsinghua University</span>
              <span class="author-block"><sup>5</sup>University of Auckland</span>
              <span class="author-block"><sup>6</sup>Bocconi University</span>
              <span class="author-block"><sup>7</sup>The University of Melbourne</span>
            </div>

          </div>
        </div>


        <!-- <br /> -->

        <!--           <div class="is-size-5 publication-authors">
            <b>Sunday July 9 14:00 - 17:30 (EDT) @ Metropolitan West</b>
          </div> -->

        <!--           <div class="is-size-5 publication-authors">
            Visit <a target="_blank" href="https://us06web.zoom.us/rec/play/6fqU9YDLoFtWqpk8w8I7oFrszHKW6JkbPVGgHsdPBxa69ecgCxbmfP33asLU3DJ74q5BXqDGR2ycOTFk.93teqylfi_uiViNK?canPlayFromShare=true&from=share_recording_detail&continueMode=true&componentName=rec-play&originRequestUrl=https%3A%2F%2Fus06web.zoom.us%2Frec%2Fshare%2FNrYheXPtE5zOlbogmdBg653RIu7RBO1uAsYH2CZt_hacD1jOHksRahGlERHc_Ybs.KGX1cRVtJBQtJf0o">this link</a>
            for the Zoom recording of the tutorial
          </div> -->

        <!--           <div class="is-size-5 publication-authors">
            QnA: <a href="https://tinyurl.com/retrieval-lm-tutorial" target="_blank"><b>tinyurl.com/retrieval-lm-tutorial</b></a>
          </div> -->


      </div>
    </div>
    </div>
    </div>
  </section>
  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">About this tutorial</h2>
          <div class="content has-text-justified">
            <p>
              This tutorial will provide an in-depth exploration of safety issues for generative AI models, covering a
              broad range of sub-topics, including a risk taxonomy, adversarial attack types, safety evaluation, defense
              mechanisms, red teaming for multi-modal models, and agentic AI. The goal is to brief attendees on the
              latest advancements and emerging trends in the field, enabling them to identify and mitigate
              vulnerabilities in generative AI systems. Participants will gain insights into recent research
              developments, open research questions, and novel research directions. This cutting-edge tutorial is
              designed for AI researchers, developers, and security professionals with a basic knowledge of red teaming
              and AI safety.
            </p>
          </div>
        </div>
      </div>

    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Schedule</h2>
          <!--         <p>
          Our tutorial will be held on July 9 (all the times are based on EDT = Toronto local time).
          <em>Slides may be subject to updates.</em>
        </p> -->

          <div class="content has-text-justified">

            <style type="text/css">
              .tg {
                border-collapse: collapse;
                border-spacing: 0;
              }

              .tg td {
                border-color: black;
                border-style: solid;
                border-width: 1px;
                font-family: Arial, sans-serif;
                font-size: 14px;
                overflow: hidden;
                padding: 10px 5px;
                word-break: normal;
              }

              .tg th {
                border-color: black;
                border-style: solid;
                border-width: 1px;
                font-family: Arial, sans-serif;
                font-size: 14px;
                font-weight: normal;
                overflow: hidden;
                padding: 10px 5px;
                word-break: normal;
              }

              .tg .tg-0pky {
                border-color: inherit;
                text-align: left;
                vertical-align: top
              }

              .tg .tg-0lax {
                text-align: left;
                vertical-align: top
              }
            </style>
            <table class="tg">
              <thead>
                <tr>
                  <th class="tg-0pky">Time</th>
                  <th class="tg-0lax">Section</th>
                  <th class="tg-0lax">Topics</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td class="tg-0lax">9:00—9:15</td>
                  <td class="tg-0lax">Introduction</td>
                  <td class="tg-0lax">
                    • Definitions and concepts of jailbreaking, red teaming, adversarial attacks<br />
                    • Importance of red teaming for generative AI
                  </td>
                </tr>
                <tr>
                  <td class="tg-0lax">9:15—9:45</td>
                  <td class="tg-0lax">Risk Taxonomization</td>
                  <td class="tg-0lax">
                    • Ethical and social risks<br />
                    • Harm types and targets<br />
                    • Domain-specific risks<br />
                    • Policy-oriented risks; case study of Meta, OpenAI, and Google policies
                  </td>
                </tr>
                <tr>
                  <td class="tg-0lax">9:45—10:15</td>
                  <td class="tg-0lax">Attack</td>
                  <td class="tg-0lax">
                    • Problem Statement<br />
                    • Attack as a search problem<br />
                    • Search Goal<br />
                    • Search Strategy
                  </td>
                </tr>
                <tr>
                  <td class="tg-0lax">10:15—10:55</td>
                  <td class="tg-0lax">Attack Methods</td>
                  <td class="tg-0lax">
                    • Completion compliance<br />
                    • Instruction indirection<br />
                    • Generalization glide<br />
                    • Model manipulation
                  </td>
                </tr>
                <tr>
                  <td class="tg-0lax">10:55—11:25</td>
                  <td class="tg-0lax">Evaluation Benchmarks</td>
                  <td class="tg-0lax">
                    • How to define a successful attack<br />
                    • Balance between helpfulness and harmlessness<br />
                    • Common datasets and metrics
                  </td>
                </tr>
                <tr>
                  <td class="tg-0lax">11:25—11:55</td>
                  <td class="tg-0lax">Defense Mechanisms</td>
                  <td class="tg-0lax">
                    • Training-time defenses<br />
                    • Inference-time defenses
                  </td>
                </tr>
                <tr>
                  <td class="tg-0lax">11:55—12:25</td>
                  <td class="tg-0lax">Multi-modal and agentic AI red teaming</td>
                  <td class="tg-0lax">
                    • Recent work<br />
                    • Multi-modal model vulnerabilities<br />
                    • Red teaming strategies for agentic AI
                  </td>
                </tr>
              </tbody>
            </table>
          </div>
        </div>
      </div>

      <!-- Concurrent Work. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Target Audience</h2>
          <div class="content has-text-justified">
            <p>
              This tutorial is designed for AI researchers, developers, and security professionals who have a basic
              understanding of red teaming and AI safety. Attendees should have prior knowledge of basic AI and machine
              learning concepts, including familiarity with model training and evaluation. Knowledge of specific
              modeling approaches such as natural language processing and computer vision will be advantageous.
            </p>
          </div>

          <h2 class="title is-3">Reading List</h2>
          <div class="content">
            <ul>
              <li>
                <a href="#">Ethical and social risks of harm from language models</a> (Weidinger et al., 2021)
              </li>
              <li>
                <a href="#">A general language assistant as a laboratory for alignment</a> (Askell et al., 2021)
              </li>
              <li>
                <a href="#">Do-Not-Answer: A Dataset for Evaluating Safeguards in LLMs</a> (Wang et al., 2023)
              </li>
              <li>
                <a href="#">"Do Anything Now": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large
                  Language Models</a> (Shen et al., 2023)
              </li>
              <li>
                <a href="#">Universal and Transferable Adversarial Attacks on Aligned Language Models</a> (Andy et al.,
                2023)
              </li>
              <li>
                <a href="#">Llama Guard: LLM-based Input-Output Safeguard for Human-AI Conversations</a> (Inan et al.,
                2023)
              </li>
              <li>
                <a href="#">Against The Achilles' Heel: A Survey on Red Teaming for Generative Models</a> (Lin et al.,
                2024)
              </li>
            </ul>
          </div>
        </div>
      </div>
  </section>


  <!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      Add citation
}
</code></pre>
  </div>
</section> -->


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://github.com/ACL2023-Retrieval-LM" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              Website <a href="https://github.com/nerfies/nerfies.github.io">source code.</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>