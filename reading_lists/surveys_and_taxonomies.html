<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Safety Issues for Generative AI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>COLING 2025 Tutorial: Safety Issues for Generative AI</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="../static/css/bulma.min.css">
  <link rel="stylesheet" href="../static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="../static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="../static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="../static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="../static/js/fontawesome.all.min.js"></script>
  <script src="../static/js/bulma-carousel.min.js"></script>
  <script src="../static/js/bulma-slider.min.js"></script>
  <script src="../static/js/index.js"></script>
</head>

<body>



  <section class="section">
    <div class="container is-max-desktop">

      <!-- Concurrent Work. -->
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2>Surveys</h2>
          <ul>
            <li><a href="https://arxiv.org/abs/2401.05459">Personal LLM Agents: Insights and Survey about the
                Capability,Efficiency and Security</a></li>
            <li><a href="https://arxiv.org/abs/2401.05561">TrustLLM: Trustworthiness in Large Language Models</a></li>
            <li><a href="https://arxiv.org/abs/2401.05778">Risk Taxonomy,Mitigation,and Assessment Benchmarks of Large
                Language Model Systems</a></li>
            <li><a href="https://arxiv.org/abs/2402.00888">Security and Privacy Challenges of Large Language Models: A
                Survey</a></li>
          </ul>

          <h3>Surveys on Attacks</h3>
          <ul>
            <li><a href="https://www.mdpi.com/2079-9292/13/5/842">Robust Testing of AI Language Model Resiliency with
                Novel Adversarial Prompts</a></li>
            <li><a href="https://arxiv.org/abs/2403.16432">Don't Listen To Me: Understanding and Exploring Jailbreak
                Prompts of Large Language Models</a></li>
            <li><a href="https://arxiv.org/abs/2403.04786">Breaking Down the Defenses: A Comparative Survey of Attacks
                on Large Language Models</a></li>
            <li><a href="https://arxiv.org/abs/2402.13457">LLM Jailbreak Attack versus Defense Techniques -- A
                Comprehensive Study</a></li>
            <li><a href="https://arxiv.org/abs/2402.00898">An Early Categorization of Prompt Injection Attacks on Large
                Language Models</a></li>
            <li><a href="https://arxiv.org/abs/2402.05668">Comprehensive Assessment of Jailbreak Attacks Against
                LLMs</a></li>
            <li><a href="https://arxiv.org/abs/2308.03825">"Do Anything Now": Characterizing and Evaluating In-The-Wild
                Jailbreak Prompts on Large Language Models</a></li>
            <li><a href="https://arxiv.org/abs/2310.10844">Survey of Vulnerabilities in Large Language Models Revealed
                by Adversarial Attacks</a></li>
            <li><a href="https://arxiv.org/abs/2311.16119">Ignore This Title and HackAPrompt: Exposing Systemic
                Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition</a></li>
            <li><a href="https://arxiv.org/abs/2310.19737">Adversarial Attacks and Defenses in Large Language Models:
                Old and New Threats</a></li>
            <li><a href="https://arxiv.org/abs/2305.14965">Tricking LLMs into Disobedience: Formalizing,Analyzing,and
                Detecting Jailbreaks</a></li>
            <li><a href="https://arxiv.org/abs/2311.06237">Summon a Demon and Bind it: A Grounded Theory of LLM Red
                Teaming in the Wild</a></li>
            <li><a href="https://arxiv.org/abs/2312.10982">A Comprehensive Survey of Attack
                Techniques,Implementation,and Mitigation Strategies in Large Language Models</a></li>
            <li><a href="https://arxiv.org/abs/2311.11796">Beyond Boundaries: A Comprehensive Survey of Transferable
                Attacks on AI Systems</a></li>
          </ul>

          <h3>Surveys on Risks</h3>
          <ul>
            <li><a href="https://arxiv.org/abs/2403.13309">Mapping LLM Security Landscapes: A Comprehensive Stakeholder
                Risk Assessment Proposal</a></li>
            <li><a href="https://arxiv.org/abs/2403.12503">Securing Large Language Models: Threats,Vulnerabilities and
                Responsible Practices</a></li>
            <li><a href="https://arxiv.org/abs/2310.10383">Privacy in Large Language Models: Attacks,Defenses and Future
                Directions</a></li>
            <li><a href="https://arxiv.org/abs/2305.08005">Beyond the Safeguards: Exploring the Security Risks of
                ChatGPT</a></li>
            <li><a href="https://arxiv.org/abs/2302.0927">Towards Safer Generative Language Models: A Survey on Safety
                Risks,Evaluations,and Improvements</a></li>
            <li><a href="https://arxiv.org/abs/2308.12833">Use of LLMs for Illicit Purposes: Threats,Prevention
                Measures,and Vulnerabilities</a></li>
            <li><a href="https://arxiv.org/abs/2307.00691">From ChatGPT to ThreatGPT: Impact of Generative AI in
                Cybersecurity and Privacy</a></li>
            <li><a href="https://arxiv.org/abs/2311.16153">Identifying and Mitigating Vulnerabilities in LLM-Integrated
                Applications</a></li>
            <li><a href="https://ace.ewapublishing.org/media/50a3b1ae25cc43f4b2a62ca50409a1e2.marked.pdf">The power of
                generative AI in cybersecurity: Opportunities and challenges</a></li>
          </ul>

          <h2>Taxonomies</h2>
          <ul>
            <li><a href="https://arxiv.org/abs/2402.1402">Coercing LLMs to do and reveal (almost) anything</a></li>
            <li><a href="https://arxiv.org/abs/2310.13595">The History and Risks of Reinforcement Learning and Human
                Feedback</a></li>
            <li><a href="https://arxiv.org/abs/2310.19181">From Chatbots to PhishBots? -- Preventing Phishing scams
                created using ChatGPT,Google Bard and Claude</a></li>
            <li><a href="https://arxiv.org/abs/2305.1386">Jailbreaking ChatGPT via Prompt Engineering: An Empirical
                Study</a></li>
            <li><a href="https://arxiv.org/abs/2305.05133">Generating Phishing Attacks using ChatGPT</a></li>
            <li><a href="https://arxiv.org/abs/2303.05453">Personalisation within bounds: A risk taxonomy and policy
                framework for the alignment of large language models with personalised feedback</a></li>
            <li><a href="https://arxiv.org/abs/2308.14752">AI Deception: A Survey of Examples,Risks,and Potential
                Solutions</a></li>
            <li><a href="https://arxiv.org/abs/2311.11415">A Security Risk Taxonomy for Large Language Models</a></li>
          </ul>
        </div>
  </section>


  <!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      Add citation
}
</code></pre>
  </div>
</section> -->


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="https://github.com/ACL2023-Retrieval-LM" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              Website <a href="https://github.com/nerfies/nerfies.github.io">source code.</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>