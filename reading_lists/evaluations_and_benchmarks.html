<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description" content="Safety Issues for Generative AI">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>COLING 2025 Tutorial: Safety Issues for Generative AI</title>


    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="../static/css/bulma.min.css">
    <link rel="stylesheet" href="../static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="../static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="../static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="../static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="../static/js/fontawesome.all.min.js"></script>
    <script src="../static/js/bulma-carousel.min.js"></script>
    <script src="../static/js/bulma-slider.min.js"></script>
    <script src="../static/js/index.js"></script>
</head>

<body>



    <section class="section">
        <div class="container is-max-desktop">
            <!-- Concurrent Work. -->
            <div class="columns is-centered">
                <div class="column is-full-width">
                    <h2 class="title is-3">Evaluation Metrics</h2>

                    <h3 class="title is-5">Attack Metrics</h3>
                    <ul>
                        <li><a href="https://arxiv.org/abs/2401.00991">A Novel Evaluation Framework for Assessing
                                Resilience Against Prompt Injection Attacks in Large Language Models</a></li>
                        <li><a href="https://arxiv.org/abs/2401.09002">AttackEval: How to Evaluate the Effectiveness of
                                Jailbreak Attacking on Large Language Models</a></li>
                        <li><a href="https://arxiv.org/abs/2404.06407">Take a Look at it! Rethinking How to Evaluate
                                Language Model Jailbreak</a></li>
                    </ul>

                    <h3 class="title is-5">Defense Metrics</h3>
                    <ul>
                        <li><a href="https://arxiv.org/abs/2402.15302">How (un)ethical are instruction-centric responses
                                of LLMs? Unveiling the vulnerabilities of safety guardrails to harmful queries</a></li>
                        <li><a href="https://arxiv.org/abs/2401.00287">The Art of Defending: A Systematic Evaluation and
                                Analysis of LLM Defense Strategies on Safety and Over-Defensiveness</a></li>
                    </ul>

                    <h2 class="title is-3">Evaluation Benchmarks</h2>
                    <ul>
                        <li><a href="https://arxiv.org/abs/2404.01318">JailbreakBench: An Open Robustness Benchmark for
                                Jailbreaking Large Language Models</a></li>
                        <li><a href="https://arxiv.org/abs/2404.05399">SafetyPrompts: a Systematic Review of Open
                                Datasets for Evaluating and Improving Large Language Model Safety</a></li>
                        <li><a href="https://arxiv.org/abs/2403.13213">From Representational Harms to Quality-of-Service
                                Harms: A Case Study on Llama 2 Safety Safeguards</a></li>
                        <li><a href="https://arxiv.org/abs/2402.05044">SALAD-Bench: A Hierarchical and Comprehensive
                                Safety Benchmark for Large Language Models</a></li>
                        <li><a href="https://arxiv.org/abs/2402.1026">A StrongREJECT for Empty Jailbreaks</a></li>
                        <li><a href="https://arxiv.org/abs/2402.04249">HarmBench: A Standardized Evaluation Framework
                                for Automated Red Teaming and Robust Refusal</a></li>
                        <li><a href="https://arxiv.org/abs/2309.07045">SafetyBench: Evaluating the Safety of Large
                                Language Models with Multiple Choice Questions</a></li>
                        <li><a href="https://arxiv.org/abs/2308.01263">XSTest: A Test Suite for Identifying Exaggerated
                                Safety Behaviours in Large Language Models</a></li>
                        <li><a href="https://arxiv.org/abs/2308.13387">Do-Not-Answer: A Dataset for Evaluating
                                Safeguards in LLMs</a></li>
                        <li><a href="https://arxiv.org/abs/2304.10436">Safety Assessment of Chinese Large Language
                                Models</a></li>
                        <li><a href="https://arxiv.org/abs/2209.07858">Red Teaming Language Models to Reduce Harms:
                                Methods,Scaling Behaviors,and Lessons Learned</a></li>
                        <li><a href="https://arxiv.org/abs/2306.11247">DICES Dataset: Diversity in Conversational AI
                                Evaluation for Safety</a></li>
                        <li><a href="https://arxiv.org/abs/2307.08487">Latent Jailbreak: A Benchmark for Evaluating Text
                                Safety and Output Robustness of Large Language Models</a></li>
                        <li><a href="https://arxiv.org/abs/2311.01011">Tensor Trust: Interpretable Prompt Injection
                                Attacks from an Online Game</a></li>
                        <li><a href="https://arxiv.org/abs/2311.04235">Can LLMs Follow Simple Rules?</a></li>
                        <li><a href="https://arxiv.org/abs/2311.0837">SimpleSafetyTests: a Test Suite for Identifying
                                Critical Safety Risks in Large Language Models</a></li>
                        <li><a href="https://arxiv.org/abs/2312.14197">Benchmarking and Defending Against Indirect
                                Prompt Injection Attacks on Large Language Models</a></li>
                        <li><a href="https://arxiv.org/abs/2310.05818">SC-Safety: A Multi-round Open-ended Question
                                Adversarial Safety Benchmark for Large Language Models in Chinese</a></li>
                        <li><a href="https://arxiv.org/abs/2311.14966">Walking a Tightrope -- Evaluating Large Language
                                Models in High-Risk Domains</a></li>
                    </ul>
                </div>
    </section>


    <!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      Add citation
}
</code></pre>
  </div>
</section> -->


    <footer class="footer">
        <div class="container">
            <div class="content has-text-centered">
                <a class="icon-link" href="https://github.com/ACL2023-Retrieval-LM" class="external-link" disabled>
                    <i class="fab fa-github"></i>
                </a>
            </div>
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content">
                        <p>
                            This website is licensed under a <a rel="license"
                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                                Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>
                        <p>
                            Website <a href="https://github.com/nerfies/nerfies.github.io">source code.</a>
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>